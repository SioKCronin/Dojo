{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interview Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources:\n",
    "\n",
    "* http://www.kdnuggets.com/2016/02/21-data-science-interview-questions-answers.html\n",
    "* https://www.dezyre.com/article/100-data-science-interview-questions-and-answers-general-for-2017/184\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Describe a data project you worked on recently"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have spent the past several months building projects with Udacity, and one of my favorites involved working with Enron corporate e-mail and database data. The goal of the project was to identify and train features for a machine learning algorithm that would help identify persons of interest involved in the Enron financial scandel. \n",
    "\n",
    "\n",
    "This kind of project is precisely what excites me about leveraging machine learning to gain data insights. It's not just about firing up an algorithm and hoping it answers all our questions, but rather collaborating with the computational power of these supervised, semi-supervised, and unsupervised algorithms, and using the insights they create to refine our research targets and surface greater understanding of the phenomena our data represents.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. You are given a ten piece box of chocolate truffles. You know based on the label that six of the pieces have an orange cream filling and four of the pieces have a coconut filling. If you were to eat four pieces in a row, what is the probability that the first two pieces you eat have an orange cream filling and the last two have a coconut filling? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow-up question: If you were given an identical box of chocolates and again eat four pieces in a row, what is the probability that exactly two contain coconut filling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SQL question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SELECT state, number_of_active_users SUM(active) FROM users GROUP BY state; "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define a function first_unique that takes a string as input and returns the first non-repeated (unique) character in the input string. If there are no unique characters return None. Note: Your code should be in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-4a767c9039d0>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-4a767c9039d0>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    > first_unique('aabbcdd123')\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def first_unique(string):\n",
    " # Your code here\n",
    " return unique_char\n",
    "\n",
    "> first_unique('aabbcdd123')\n",
    "> c\n",
    "\n",
    "> first_unique('a')\n",
    "> a\n",
    "\n",
    "> first_unique('112233')\n",
    "> None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. What are underfitting and overfitting in the context of Machine Learning? How might you balance them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. If you were to start your data analyst position today, what would be your goals a year from now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A year from now, my priorities will continue to be shaped by the goals of this company, but my personal facility in selecting models that best fist the questions we are asking will have dramatically increased due to my engagement with our data. My goals a year from now will "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Explain what regularization is and why it is useful. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Regularization is the process of adding a tuning parameter to a model to induce smoothness in order to prevent overfitting. (see also KDnuggets posts on Overfitting). This is most often done by adding a constant multiple to an existing weight vector. This constant is often either the L1 (Lasso) or L2 (ridge), but can in actuality can be any norm. The model predictions should then minimize the mean of the loss function calculated on the regularized training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Which data scientists do you admire most? which company teams?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. How would you validate a model you created to generate a predictive model of a quantitative outcome variable using multiple regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proposed methods for model validation: \n",
    "\n",
    "* If the values predicted by the model are far outside of the response variable range, this would immediately indicate poor estimation or model inaccuracy.\n",
    "* If the values seem to be reasonable, examine the parameters; any of the following would indicate poor estimation or multi-collinearity: opposite signs of expectations, unusually large or small values, or observed inconsistency when the model is fed new data.\n",
    "* Use the model for prediction by feeding it new data, and use the coefficient of determination (R squared) as a model validity measure.\n",
    "* Use data splitting to form a separate dataset for estimating model parameters, and another for validating predictions.\n",
    "* Use jackknife resampling if the dataset contains a small number of instances, and measure validity with R squared and mean squared error (MSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Explain what precision and recall are. How do they relate to the ROC curve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine there are 100 positive cases among 10,000 cases. You want to predict which ones are positive, and you pick 200 to have a better chance of catching many of the 100 positive cases.  You record the IDs of your predictions, and when you get the actual results you sum up how many times you were right or wrong. There are four ways of being right or wrong:\n",
    "\n",
    "* TN / True Negative: case was negative and predicted negative\n",
    "* TP / True Positive: case was positive and predicted positive\n",
    "* FN / False Negative: case was positive but predicted negative\n",
    "* FP / False Positive: case was negative but predicted positive\n",
    "\n",
    "Makes sense so far? Now you count how many of the 10,000 cases fall in each bucket, say:\n",
    "* TN: 9,760\n",
    "* FP: 140\n",
    "* FN: 40\n",
    "* TP: 60\n",
    "\n",
    "Now, your boss asks you three questions:\n",
    "\n",
    "What percent of your predictions were correct? \n",
    "* You answer: the \"accuracy\" was (9,760+60) out of 10,000 = 98.2%\n",
    "\n",
    "What percent of the positive cases did you catch? \n",
    "* You answer: the \"recall\" was 60 out of 100 = 60%\n",
    "\n",
    "What percent of positive predictions were correct? \n",
    "* You answer: the \"precision\" was 60 out of 200 = 30%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. How can you prove that one improvement you've brought to an algorithm is really an improvement over not doing anything?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the exact approach to prove that one improvement you've brought to an algorithm is really an improvement over not doing anything would depend on the actual case at hand, there are a few common guidelines:\n",
    "* Ensure that there is no selection bias in test data used for performance comparison\n",
    "* Ensure that the test data has sufficient variety in order to be symbolic of real-life data (helps avoid overfitting)\n",
    "* Ensure that \"controlled experiment\" principles are followed i.e. while comparing performance, the test environment (hardware, etc.) must be exactly the same while running original algorithm and new algorithm\n",
    "* Ensure that the results are repeatable with near similar results\n",
    "* Examine whether the results reflect local maxima/minima or global maxima/minima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. What is root cause analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Root cause analysis (RCA) is a method of problem solving used for identifying the root causes of faults or problems. A factor is considered a root cause if removal thereof from the problem-fault-sequence prevents the final undesirable event from recurring; whereas a causal factor is one that affects an event's outcome, but is not a root cause."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Are you familiar with price optimization, price elasticity, inventory management, competitive intelligence? Give examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those are economics terms that are not frequently asked of Data Scientists but they are useful to know. \n",
    "\n",
    "Price optimization is the use of mathematical tools to determine how customers will respond to different prices for its products and services through different channels. \n",
    "\n",
    "Big Data and data mining enables use of personalization for price optimization. Now companies like Amazon can even take optimization further and show different prices to different visitors, based on their history, although there is a strong debate about whether this is fair. \n",
    "\n",
    "Price elasticity in common usage typically refers to\n",
    "**Price elasticity of demand**, a measure of price sensitivity. It is computed as: \n",
    "Price Elasticity of Demand = % Change in Quantity Demanded / % Change in Price.\n",
    " \n",
    "Similarly, **Price elasticity of supply** is an economics measure that shows how the quantity supplied of a good or service responds to a change in its price. \n",
    "\n",
    "**Inventory management** is the overseeing and controlling of the ordering, storage and use of components that a company will use in the production of the items it will sell as well as the overseeing and controlling of quantities of finished products for sale. \n",
    "\n",
    "**Competitive intelligence**: the action of defining, gathering, analyzing, and distributing intelligence about products, customers, competitors, and any aspect of the environment needed to support executives and managers making strategic decisions for an organization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. What is statistical power?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikipedia defines Statistical power or sensitivity of a binary hypothesis test is the probability that the test correctly rejects the null hypothesis (H0) when the alternative hypothesis (H1) is true. \n",
    "\n",
    "To put in another way, Statistical power is the likelihood that a study will detect an effect when the effect is present. The higher the statistical power, the less likely you are to make a Type II error (concluding there is no effect when, in fact, there is). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Explain what resampling methods are and why they are useful. Also explain their limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classical statistical parametric tests compare observed statistics to theoretical sampling distributions. Resampling is a data-driven, not theory-driven methodology which is based upon repeated sampling within the same sample. \n",
    "\n",
    "Resampling refers to methods for doing one of these\n",
    "* Estimating the precision of sample statistics (medians, variances, percentiles) by using subsets of available data (jackknifing) or drawing randomly with replacement from a set of data points (bootstrapping)\n",
    "* Exchanging labels on data points when performing significance tests (permutation tests, also called exact tests, randomization tests, or re-randomization tests)\n",
    "* Validating models by using random subsets (bootstrapping, cross validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Is it better to have too many false positives, or too many false negatives? Explain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It depends on the question as well as on the domain for which we are trying to solve the question. \n",
    "\n",
    "In medical testing, false negatives may provide a falsely reassuring message to patients and physicians that disease is absent, when it is actually present. This sometimes leads to inappropriate or inadequate treatment of both the patient and their disease. So, it is desired to have too many false positive. \n",
    "\n",
    "For spam filtering, a false positive occurs when spam filtering or spam blocking techniques wrongly classify a legitimate email message as spam and, as a result, interferes with its delivery. While most anti-spam tactics can block or filter a high percentage of unwanted emails, doing so without creating significant false-positive results is a much more demanding task. So, we prefer too many false negatives over many false positives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. What is selection bias, why is it important and how can you avoid it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selection bias, in general, is a problematic situation in which error is introduced due to a non-random population sample. For example, if a given sample of 100 test cases was made up of a 60/20/15/5 split of 4 classes which actually occurred in relatively equal numbers in the population, then a given model may make the false assumption that probability could be the determining predictive factor. Avoiding non-random samples is the best way to deal with bias; however, when this is impractical, techniques such as resampling, boosting, and weighting are strategies which can be introduced to help deal with the situation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Explain what is overfitting and how would you control for it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overfitting is finding spurious results that are due to chance and cannot be reproduced by subsequent studies. \n",
    "\n",
    "We frequently see newspaper reports about studies that overturn the previous findings, like eggs are no longer bad for your health, or saturated fat is not linked to heart disease. The problem, in our opinion is that many researchers, especially in social sciences or medicine, too frequently commit the cardinal sin of Data Mining - Overfitting the data. \n",
    "\n",
    "The researchers test too many hypotheses without proper statistical control, until they happen to find something interesting and report it.  Not surprisingly, next time the effect, which was (at least partly) due to chance, will be much smaller or absent. \n",
    "\n",
    "These flaws of research practices were identified and reported by John P. A. Ioannidis in his landmark paper Why Most Published Research Findings Are False (PLoS Medicine, 2005). Ioannidis found that very often either the results were exaggerated or the findings could not be replicated. In his paper, he presented statistical evidence that indeed most claimed research findings are false. \n",
    "\n",
    "Ioannidis noted that in order for a research finding to be reliable, it should have:\n",
    "* Large sample size and with large effects\n",
    "* Greater number of and lesser selection of tested relationship\n",
    "* Greater flexibility in designs, definitions, outcomes, and analytical modes\n",
    "* Minimal bias due to financial and other factors (including popularity of that scientific field)\n",
    "\n",
    "Several methods can be used to avoid \"overfitting\" the data\n",
    "* Try to find the simplest possible hypothesis\n",
    "* Regularization (adding a penalty for complexity)\n",
    "* Randomization Testing (randomize the class variable, try your method on this data - if it find the same strong results, something is wrong)\n",
    "* Nested cross-validation  (do feature selection on one level, then run entire method in cross-validation on outer level)\n",
    "* Adjusting the False Discovery Rate\n",
    "* Using the reusable holdout method - a breakthrough approach proposed in 2015\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 19. Give an example of how you would use experimental design to answer a question about user behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Formulate the Research Question: \n",
    "What are the effects of page load times on user satisfaction ratings? \n",
    "\n",
    "Step 2: Identify variables: \n",
    "We identify the cause & effect. Independent variable -page load time, Dependent variable- user satisfaction rating \n",
    "\n",
    "Step 3: Generate Hypothesis: \n",
    "Lower page download time will have more effect on the user satisfaction rating for a web page. Here the factor we analyze is page load time. \n",
    "\n",
    "Step 4: Determine Experimental Design. \n",
    "We consider experimental complexity i.e vary one factor at a time or multiple factors at one time in which case we use factorial design (2^k design). A design is also selected based on the type of objective (Comparative, Screening, Response surface) & number of factors. \n",
    "\n",
    "Here we also identify within-participants, between-participants, and mixed model.For e.g.: There are two versions of a page, one with Buy button (call to action) on left and the other version has this button on the right. \n",
    "\n",
    "Within-participants design - both user groups see both versions. \n",
    "\n",
    "Between-participants design - one group of users see version A & the other user group version B. \n",
    "\n",
    "Step 5: Develop experimental task & procedure: \n",
    "Detailed description of steps involved in the experiment, tools used to measure user behavior, goals and success metrics should be defined. Collect qualitative data about user engagement to allow statistical analysis. \n",
    "\n",
    "Step 6: Determine Manipulation & Measurements \n",
    "\n",
    "Manipulation: One level of factor will be controlled and the other will be manipulated. We also identify the behavioral measures:\n",
    "Latency- time between a prompt and occurrence of behavior (how long it takes for a user to click buy after being presented with products).\n",
    "Frequency- number of times a behavior occurs (number of times the user clicks on a given page within a time)\n",
    "Duration-length of time a specific behavior lasts(time taken to add all products)\n",
    "Intensity-force with which a behavior occurs ( how quickly the user purchased a product)\n",
    "\n",
    "\n",
    "Step 7: Analyze results \n",
    "Identify user behavior data and support the hypothesis or contradict according to the observations made for e.g. how majority of users satisfaction ratings compared with page load times. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20. What is the difference between \"long\" (\"tall\") and \"wide\" format data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most data mining / data science applications there are many more records (rows) than features (columns) - such data is sometimes called \"tall\" (or \"long\") data. \n",
    "\n",
    "In some applications like genomics or bioinformatics you may have only a small number of records (patients), eg 100, but perhaps 20,000 observations for each patient. The standard methods that work for \"tall\" data will lead to overfitting the data, so special approaches are needed. \n",
    "\n",
    "The problem is not just reshaping the data (here there are useful R packages), but avoiding false positives by reducing the number of features to find most relevant ones. \n",
    "\n",
    "Approaches for feature reduction like Lasso are well covered in Statistical Learning with Sparsity: The Lasso and Generalizations, by Hastie, Tibshirani, and Wainwright. (you can download free PDF of the book) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 21. Give an example of Freedman's Paradox?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22. How would you screen for outliers and what should you do if you find one?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some methods to screen outliers are z-scores, modified z-score, box plots, Grubb's test, Tietjen-Moore test exponential smoothing, Kimber test for exponential distribution and moving window filter algorithm. However two of the robust methods in detail are: \n",
    "\n",
    "Inter Quartile Range \n",
    "An outlier is a point of data that lies over 1.5 IQRs below the first quartile (Q1) or above third quartile (Q3) in a given data set.\n",
    "High = (Q3) + 1.5 IQR\n",
    "Low = (Q1) - 1.5 IQR\n",
    "\n",
    " \n",
    "Tukey Method \n",
    "\n",
    "It uses interquartile range to filter very large or very small numbers. It is practically the same method as above except that it uses the concept of \"fences\". The two values of fences are:\n",
    "Low outliers = Q1 - 1.5(Q3 - Q1) = Q1 - 1.5(IQR)\n",
    "High outliers = Q3 + 1.5(Q3 - Q1) = Q3 + 1.5(IQR)\n",
    "\n",
    " \n",
    "Anything outside of the fences is an outlier. \n",
    "\n",
    "When you find outliers, you should not remove it without a qualitative assessment because that way you are altering the data and making it no longer pure. It is important to understand the context of analysis or importantly \"The Why question - Why an outlier is different from other data points?\" \n",
    "\n",
    "This reason is critical. If outliers are attributed to error, you may throw it out but if they signify a new trend, pattern or reveal a valuable insight into the data you should retain it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
